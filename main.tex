\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[english]{babel}

\usepackage[style=ieee, maxbibnames=5, backend=biber, url=false]{biblatex}
\addbibresource{MyLibrary.bib}
\usepackage{hyperref} % for hyperlinks
\usepackage[capitalise, noabbrev]{cleveref} % for \cref, MUST be loaded AFTER hyperref

\begin{document}

\section{Introduction}
% Option 2:

% RL in POMDPs is hard
Applying deep reinforcement learning (RL) algorithms to partially observable Markov decision processes (POMDPs) is difficult. 
One reason is poor sample efficiency, which is  already in issue in fully observable MDPs, but is exacerbated by partial observability. 
Second, principal reason, is state aliasing; where in POMDPs the same observations correspond to different environmental states.  

% Model-based RL helps
Model-based RL algorithms, such as MuZero~\cite{schrittwieserMasteringAtariGo2020}, are generally more sample efficient by constructing a model of the environment. 
This model is then used to supplement the agent's experience in the environment to plan actions. 
However, POMDPs pose a difficult problem for RL algorithms, as discovering the right model is much more challenging task in POMDPs than it is in MDPs. 

% Limitations of MuZero
Original MuZero algorithm is a model-based reinforcement learning algorithm that learns a model of the environment and uses it to plan and learn policies. 
It excels in deterministic, fully observable domains.
It also performs well with small amount of stochasticity, but the Stochastic MuZero~\cite{antonoglouPlanningStochasticEnvironments2021} algorithm obtains slightly better results~\cite{niuLightZeroUnifiedBenchmark2023}. 
This is also true in partially observable domains.
However, in general, MuZero, nor Stochastic MuZero are able to perform well in partially observable domains, as they lack mechanisms to propagate information collected from previous observations to the future decisions in the same episode. 

% MuZero and SLAMuZero
One option is to provide an algorithm with prior understanding of how the model should look like.
In this work, we are inspired by the SLAMuZero~\cite{fangSLAMuZeroPlanLearn2024} algorithm intended for the SLAM problem in robotics. 
In training, the SLAMuZero agent is provided with maps of the training environments, which provide an engineered abstraction for the agent to use to learn a model of the environment.
When the agent is deployed in a new environment, it uses the learned model to create a new map, based on the prior understanding of how the environmental model should look like.
However, the SLAMuZero algorithm is also missing a mechanism for keeping track of information collected in previous timesteps, which is essential for effective planning in partially observable domains.
% SLAMuZero is a variant of the MuZero algortihm that is able to reconstruct a map of an unknown environment while navigating through it.
% In SLAMuZero, the agent is provided with a number of training environments and their maps, which the agent uses to train a model that can create a map of a new environment.

% Hypothesis
We hypothesize that for efficiently extending the MuZero algorithms to partially observable domains, two things are needed. 
First, information collected in previous timesteps needs to propagate to the future decisions in the same episode. This can be achieved by using the previous latent space as additional input to the the representation network, which is the component responsibnle for encoding observations into the latent space.
However, this is not enough, as training such representation network is sample inefficient; with partial observations,  latent space may take a long time to learn a representation of the environment that is actually useful for planning, even though the actual environment model migh be simple.  
This can be helped by guiding the construction of the latent space in some way. 
We do this by having the model reconstruct a known, useful abstraction of the environment, such as map, from the latent states. 
This forces the latent space to encode information critical for planning in the environment, improving sample efficiency. 

In some domains, we have existing, readily available abstractions of the environment that we know to be useful for planning, such as maps in gridworld mazes. 
These abstractions, when used as a training target for the reconstruction from the latent state space, will force the agent to focus on learning the latent state space that encodes this information, and as a result, will converge faster. 
While using engineered abstractions goes against the idea of the bitter lesson, it is not necessary to use the abstractions for the whole training process, but only to shape the initial latent space.

In this work, we propose and investigate a novel architecture that extends MuZero. Our method incorporates an explicit memory mechanism to handle state aliasing and utilizes a map-reconstruction task to guide the latent space towards representations that are immediately useful for planning.
% We demonstrate that this combination leads to faster convergence and superior performance in challenging, partially observable environments.


\section{Literature Survey}


\section{Hypothesis}
The main hypothesis for this work is that that we can improve the performance of the MuZero algorithm in partially observable domains by recurrent representation network, and by guiding the construction of the latent space with a prior knowledge of how the environment abstraction looks like.

We have three main hypothesis:
\begin{itemize}
    \item \textbf{H1: Recurrent Representation} Making the representation network recurrent will allow the agent to keep track of the information collected in previous timesteps, allowing the MuZero agent to operate in partially observable domains. This is required for the agent to be able to compress observations into beliefs about the environment, which is essential for effective planning in partially observable domains.
    \item \textbf{H2: Map Abstraction} The "map reconstruction" task will guide the latent space towards representations that are useful for planning, improving sample efficiency.
    \item \textbf{H3: Bias} The combination of recurrent representation network and map reconstruction task leads to faster convergence and superior performance in in partially observable environments. However, assuming that the map is not an optimal abstraction, the map reconstruction loss biases the latent space. 
\end{itemize}

Baseline for the experiments is the original MuZero algorithm. For testing H1, we will use the MuZero algorithm with recurrent representation network, the MuZeroRec algorithm. For testing H2, we will use the MuZeroRec algorithm with the map reconstruction task, the MuZeroRecMap algorithm. For testing H3, we will compare the performance of the MuZeroRecMap algorithm with the MuZeroRec algorithm. 

For H1, we want to show improved performance in terms of collected reward of the MuZero and MuZeroRec algorithms. This is because training the recurrent representation need not be more efficient, and will require training on full epiosodes instead of transitions. As such, it would be unfair to compare the sample efficiency to the baseline. 

For H2, we want to show that the MuZeroRecMap algorithm is more sample efficient that the MuZeroRec algorithm, so we will be looking at the number of episodes needed to reach the same performance level.
Note, that it would be difficult to test H2 by applying map reconstruction to original MuZero, as is done by~\cite{fangSLAMuZeroPlanLearn2024}; in evaluation, we would be asking the agent to construct the whole map of the environment from each observation, without the ability to use the previous observations. 

However, we assume that introducing the map reconstruction loss restrains the MuZeroRec algortihm from finding the best possible abstraction. 
Therefore to test H3, we will select domain sizes where we can run MuZeroRec and MuZeroRecMap to convergence, and compare their performance.  
We expect the MuZeroRecMap algorithm to achieve good performance faster, but MuZeroRec should eventually outperform it. 
By exploring the tradeoff between the performance and the sample efficiency, we can then propose a method that biases the latent states in the initial stages of training, and turns of the map reconstruction loss later.

The map reconstruction network can also be useful to explore the structure of the latent space in novel environments, and to understand how the agent perceives the environment.

The experiment environment will be gridworld mazes for simplicity and because they are standard problem in RL literature and have easily defined maps that can be used as the target for the map reconstruction task.

\section{Experimental setup}
Our gridworld mazes will be based on the standard minigrid library, part of the OpenAI Gym. However, we will need to modify the mazes to fit our needs. 
What we need is following:

\begin{itemize}
    \item Partial observability: the agent will only see small part of the world.
    \item Useful mapping: we want the agent to learn the map of the environment, and we want the agent to have the opportunity to improve his performance by using the map.
    \item Stochasticity: we want the agent to be able to implicityly learn belief over the map. 
\end{itemize}

These requirements lead us to the folowing design of the training and evaluation environments. The maze contains wall cells, empty cells, single goal cell and an agent. The agent starts in predetermined position in randomly selected maze, with single goal tile positionws in the maze. Reaching the goal tile gives the agent positive reward, every step incurs a small negative reward. To force partial observability, the viewport of the agent is limited to a small area in front of the agent, 3x3 tiles, with the agent centered on one side of the viewport. This means that observations in different states will be aliased. 

To make exploaration of the map useful, wthe goal cell is not a terminal state, instead, we terminate episodes after predetermined number of steps, and we teleport the agent to a random position in the maze after reaching the goal tile. This also introduces stochastic transitions to the model, and the agent needs to deal with the uncertainty of its position in the maze after each teleportation. It can use its aliased observations to orient itself in the maze, and then use the map to plan its route to the goal tile. To reinforce the map use even more strongly, we have a deterministic movement and observatioon model with no stochasticity. 

In training, the agent will be trained on a set of randomly generated mazes, with previously unseen mazes used for evaluation.

\section{MuZeroRec and MuZeroRecMap algorithms}






\section{Results}

\printbibliography

\end{document}