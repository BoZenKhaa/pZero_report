\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[english]{babel}

\usepackage[style=ieee, maxbibnames=5, backend=biber, url=false]{biblatex}
\addbibresource{MyLibrary.bib}
\usepackage{hyperref} % for hyperlinks
\usepackage[capitalise, noabbrev]{cleveref} % for \cref, MUST be loaded AFTER hyperref

\begin{document}

\section{Introduction}


% Option 1:

% In this work, we consider the problem of forming a compact POMDP abstraction in an unknown POMDP environment. 
% In our setup, an agent is given a number of small simulated environments that may or may not represent the characteristics of the true environment. 
% In these environments, the agent has access to the underlying POMDP model, which can be used as a training target. 
% In the new environment, the goal of the agent is to explore the environment to learn an abstraction that can be used to solve the POMDP problem. 

% Option 2:

% RL in POMDPs is hard
Applying reinforcement learning (RL) algorithms to partially observable Markov decision processes (POMDPs) is difficult due to poor sample efficiency. This is already in issue in fully observable MDPs, but in POMDPs the problem is exacerbated by uncertainty about the current state and stochastic observations, which result in large (continous) branching factors. 

% Model-based RL helps
Model-based RL algorithms, such as MuZero~\cite{schrittwieserMasteringAtariGo2020}, are generally more sample efficient by constructing a model of the environment. This model is then used to supplement the agent's experience in the environment to plan actions. However, POMDPs pose a difficult problem for RL algorithms, as discovering the right model is much more challenging task in POMDPs than it is in MDPs.

% MuZero and SLAMuZero
One option is to provide an algorithm with prior understanding of how the model should look like.
In this work, we consider the SLAMuZero~\cite{fangSLAMuZeroPlanLearn2024} algorithm intended for the SLAM problem in robotics. 
In training, the SLAMuZero agent is provided with maps of the training environments, which provide an engineered abstraction for the agent to use to learn a model of the environment.
When the agent is deployed in a new environment, it uses the learned model to create a new map, based on the prior understanding of how the environmental model should look like.
% SLAMuZero is a variant of the MuZero algortihm that is able to reconstruct a map of an unknown environment while navigating through it.
% In SLAMuZero, the agent is provided with a number of training environments and their maps, which the agent uses to train a model that can create a map of a new environment.

% Limitations of MuZero
Original MuZero algorithm is a model-based reinforcement learning algorithm that learns a model of the environment and uses it to plan and learn policies. 
It excels in deterministic, fully observable domains.
It also performs well with small amount of stochasticity, but the Stochastic MuZero~\cite{antonoglouPlanningStochasticEnvironments2021} algorithm obtains slightly better results~\cite{niuLightZeroUnifiedBenchmark2023}. 
This is also true in partially observable domains.
However, in general, MuZero, nor Stochastic MuZero are able to perform well in partially observable domains, as they lack mechanisms to capture the state uncertainty, and as a result, rely on mapping observation histories to latent states.


% Hypothesis
MuZero algorithm works by learning a model of the environment in the form of the latent state space, which is then used to plan future actions. 
However, in partially observable domains MDPs, observations are noisy and do not provide a lot of information about the environment. 
As a result, the latent space may take a long time to learn a representation of the environment that is actually useful for planning, even though the actual environment model migh be simple.  
However, in some domains, we have existing, readily available abstractions of the environment that we know to be useful for planning, such as maps in gridworld mazes. These abstractions, when used as a training target for the reconstruction from the latent state space, will force the agent to focus on learning the latent state space that encodes this information, and as a result, will converge faster. 

% Hypothesis
% In this work, we focus on partially observable domains with observations that carry only a small amount of information about the state of the environment.
% We hypothesise that in such domains, the agent needs to build an internal representation of the environment to be able to plan and learn policies effectively.

% Abstractions
However, we are interested in abstractions; maps in SLAMuZero are an engineered abstraction of the environment. 
We train the mapping model in MuZero by training on specific examples, that are possibly much simpler than the actual environment. 
We hypothesize that this will allow us to shape the type of abstraction agent forms in the testing environment, speeding up the learning process.

% Experiments
% In our experiment, we will start with a simple grid world environment, starting with a planning problem with no stochasticity and full observability. 
% Then, we will gradually increase the complexity of the environment, limiting the observations, and adding stochasticity. 
% We will compare the performance of SLAMuZero with the original MuZero algorithm.

% 

\section{Literature Survey}

\section{Thoughts and Prayers}
In SLAMuZero, the agent is using the decoder network to reconstruct the map of the environment from any latent state. If trained on a single environment, the decoder can learn the whole of the map, with states holding only the information about the current position of the agent in the environment. However, if trained on multiple environments, the decoder can only encode the common features of all environments, and the states in the latent space need to hold enough compressed information about the map of the current environemnt to be able to reconstruct it.

\section{Experiments}
In the experiments, we will train both MuZero and SLAMuZero algorithms on a set of training environments. Crucially, there will be a variety of training environments, and evaluation will be performed on a new, previously unseen environment. 

In the training, the SLAMuZero agent will be provided with the maps of the training environments, which will be used to train the decoder. Some thoughts around this:

\begin{itemize}
    \item The maps provide MuZero with additional information about the environment, which is not available in the observations. This might be seen as unfair comparison.
    \begin{itemize}
        \item We could train the SLAMuZero agent with maps, but not use the maps for policy network. The idea here being that having the maps shapes the latenr space, but the agent still learns the policy from the search on the latent space only, not from the maps.
    \end{itemize}
    \item Additionally, when using the maps as policy network input as in the original SLAMuZero, the added decoder network increases the number of parameters in the whole algorithm, which might lead to better performance.
    \begin{itemize}
        \item We could keep the number of parameters in the whole algorithm the same, by removing some of the parameters from other networks.
    \end{itemize}
\end{itemize}

In the experiments, we might want to look at sample efficiecy (arguing that building maps improves it, as the maps guide the construction of the latent space positively), and performance improvement (arguing that the maps allow the agent to plan better and come up with better policies).

\paragraph{Sample Efficiency of Maps} 
When interested in sample efficiency, the hypotheis is that "the maps as target for the decoder shape the latent space to include the information needed to build the maps, which we assume is the information needed to plan effectively in the environment". 

For this experiment, we want to train both MuZero and SLAMuZero until convergence, and then compare the number of samples needed to reach the same performance level.

\paragraph{Performance Improvement with Maps}
When interested in performance improvment, we want to train both MuZero and SLAMuZero on the same number of episodes, and then show tha SLAMuZero performs better than MuZero. The hypothesis here is that "the maps allow efficient planning in the environment, which results in better policies". For this, we will need:
\begin{itemize}
    \item low-information observations, so that the agent can benefit from the construction of maps, e.g. two tiles in front of the agent.
\end{itemize}


\section{Results}

\printbibliography

\end{document}